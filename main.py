# ==========================================
# Hi Augustine â€” Streamlit RAG Chatbot
# Supabase + OpenAI Embedding + Cerebras LLM
# ==========================================

import os
import streamlit as st
from openai import OpenAI
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# 1) Supabase ì—°ê²° ì„¤ì • (ë°˜ë“œì‹œ anon key)
# ==========================================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")  # âœ” anon key only

if not SUPABASE_URL or not SUPABASE_KEY:
    st.error("âŒ SUPABASE_URL ë˜ëŠ” SUPABASE_ANON_KEYê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)


def check_supabase_connection():
    try:
        supabase.table("documents").select("id").limit(1).execute()
        return True, "ì •ìƒ ì—°ê²°ë¨"
    except Exception as e:
        return False, str(e)


# ==========================================
# 2) Cerebras LLM (ì±„íŒ… ëª¨ë¸)
# ==========================================
client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=os.getenv("CEREBRAS_API_KEY")
)

if not os.getenv("CEREBRAS_API_KEY"):
    st.error("âŒ CEREBRAS_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()


# ==========================================
# 3) OpenAI Embedding ëª¨ë¸
# ==========================================
embed_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

if not os.getenv("OPENAI_API_KEY"):
    st.error("âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()


def embed_text(text: str):
    try:
        res = embed_client.embeddings.create(
            model="text-embedding-3-large",  # 3072 vector
            input=text
        )
        return res.data[0].embedding
    except Exception as e:
        st.error(f"ì„ë² ë”© ì˜¤ë¥˜: {str(e)}")
        return None


# ==========================================
# 4) Supabase ë²¡í„° ê²€ìƒ‰ (RPC ì‚¬ìš©)
# ==========================================
def search_supabase(query_embedding, match_count=5):
    try:
        response = supabase.rpc(
            "match_documents",
            {
                "query_embedding": query_embedding,
                "match_threshold": 0.3,     # SQL í•¨ìˆ˜ì™€ ë™ì¼í•´ì•¼ í•¨
                "match_count": match_count
            }
        ).execute()
        return response.data or []
    except Exception as e:
        st.error(f"Supabase RPC ì˜¤ë¥˜: {str(e)}")
        return []


def build_context(question: str):
    emb = embed_text(question)
    if emb is None:
        return ""

    matches = search_supabase(emb, match_count=5)

    if not matches:
        return "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."

    return "\n\n".join([m["content"] for m in matches])


# ==========================================
# 5) ì‚¬ì´ë“œë°” â€” ëª¨ë¸ ì„ íƒ / DB ì—°ê²°ìƒíƒœ í‘œì‹œ
# ==========================================
st.sidebar.title("âš™ï¸ ì„¤ì •")

ok, msg = check_supabase_connection()
if ok:
    st.sidebar.success("ğŸŸ¢ Supabase ì—°ê²°ë¨")
else:
    st.sidebar.error(f"ğŸ”´ Supabase ì—°ê²° ì‹¤íŒ¨\n\n{msg}")

model_options = {
    "GPT-OSS 120B": "gpt-oss-120b",
    "QWen 32B": "qwen-3-32b",
    "LLaMA 3.1 8B": "llama3.1-8b",
}

selected_model_name = st.sidebar.selectbox(
    "ğŸ¤– LLM ì„ íƒ",
    list(model_options.keys())
)

st.session_state["llm_model"] = model_options[selected_model_name]


# ==========================================
# 6) ì‹œìŠ¤í…œ ë©”ì‹œì§€ â€” ì–´ê±°ìŠ¤í‹´ ì—­í•  ë¶€ì—¬
# ==========================================
system_prompt = """
ì—­í• : ë„ˆëŠ” íˆí¬ì˜ ì–´ê±°ìŠ¤í‹´(Augustine of Hippo)ì˜ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.
ë„¤ ë§íˆ¬ëŠ” ë”°ëœ»í•˜ê³  ê¹Šì€ í†µì°°ì„ ê°€ì§„ ëª©ì‚¬ì´ì ì² í•™ìì²˜ëŸ¼ ë§í•œë‹¤.

ë‹µë³€ ì›ì¹™:
1) ë”°ëœ»í•œ ê³µê°
2) ê¹Šì€ ì‹ í•™Â·ì² í•™ì  í†µì°°
3) ì–´ê±°ìŠ¤í‹´ ì‚¬ìƒ ë°˜ì˜
4) ì„±ê²½ì  ë¶€ë“œëŸ¬ìš´ ì„¤ëª…
5) ë¹„ê¸°ë…êµì¸ë„ í¬ìš©
6) í•µì‹¬ ìš”ì•½
7) ë§ˆì§€ë§‰ì— ë¼í‹´ì–´ í•œ ë¬¸ì¥ ìš”ì•½
8) contextì— ì—†ëŠ” ë‚´ìš©: "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."
9) ëŒ€ë‹µ ë„ì¤‘ì— ëë§ˆì¹˜ì§€ ë§ê³  ë°˜ë“œì‹œ ë§ˆë¬´ë¦¬ í•˜ê¸°.
"""


# ==========================================
# 7) LLM ì‘ë‹µ ìƒì„±
# ==========================================
def ask_llm(question: str, context: str):
    rag_prompt = f"""
[Context: Augustine ë¬¸í—Œ ë°œì·Œ]
{context}

(ì£¼ì˜: ìœ„ context ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì—¬ ë‹µí•˜ë¼.
contextì— ì—†ëŠ” ë‚´ìš©ì€ ë°˜ë“œì‹œ "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ë¼.)

ì§ˆë¬¸: {question}
"""

    try:
        completion = client.chat.completions.create(
            model=st.session_state["llm_model"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": rag_prompt}
            ],
            temperature=0.4,
            max_completion_tokens=1000
        )
        return completion.choices[0].message.content
    except Exception as e:
        st.error(f"LLM ì˜¤ë¥˜: {str(e)}")
        return "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ==========================================
# 8) UI ì¶œë ¥ â€” ëŒ€í™” ê¸°ë¡ í‘œì‹œ
# ==========================================
st.title("Hi ì–´ê±°ìŠ¤í‹´ ğŸ˜âœï¸")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": system_prompt}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])


# ==========================================
# 9) ì‚¬ìš©ì ì…ë ¥ â†’ RAG â†’ LLM ëŒ€ë‹µ
# ==========================================
if user_input := st.chat_input("ì‹ ì•™/ì‹ í•™ ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”?"):

    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    # ğŸ” RAG Context ìƒì„±
    context = build_context(user_input)

    # ğŸ¤– LLM ë‹µë³€
    answer = ask_llm(user_input, context)

    with st.chat_message("assistant"):
        st.markdown(answer)

    st.session_state.messages.append({"role": "assistant", "content": answer})


# ==========================================
# 10) ë¡œì»¬ Streamlit ì‹¤í–‰ìš©
# ==========================================
if __name__ == "__main__":
    import subprocess, sys
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])
