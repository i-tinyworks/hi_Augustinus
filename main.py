# ==========================================
# Hi Augustine â€” Streamlit RAG Chatbot
# Supabase + OpenAI Embedding + Cerebras LLM
# ==========================================

import os
import streamlit as st
from openai import OpenAI
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# 1) Supabase ì—°ê²° ì„¤ì •
# ==========================================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")  # âœ” anon key only

if not SUPABASE_URL or not SUPABASE_KEY:
    st.error("âŒ SUPABASE_URL ë˜ëŠ” SUPABASE_ANON_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def check_supabase_connection():
    try:
        supabase.table("documents").select("id").limit(1).execute()
        return True, "ì •ìƒ ì—°ê²°ë¨"
    except Exception as e:
        return False, str(e)


# ==========================================
# 2) Cerebras LLM (ì±„íŒ… ëª¨ë¸)
# ==========================================
CEREBRAS_KEY = os.getenv("CEREBRAS_API_KEY")
if not CEREBRAS_KEY:
    st.error("âŒ CEREBRAS_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=CEREBRAS_KEY
)

# ==========================================
# 3) OpenAI Embedding ëª¨ë¸
# ==========================================
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_KEY:
    st.error("âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

embed_client = OpenAI(api_key=OPENAI_KEY)

def embed_text(text: str):
    try:
        res = embed_client.embeddings.create(
            model="text-embedding-3-large",  # vector size 3072
            input=text
        )
        return res.data[0].embedding
    except Exception as e:
        st.error(f"ì„ë² ë”© ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


# ==========================================
# 4) Supabase ë²¡í„° ê²€ìƒ‰ (RPC)
# ==========================================
def search_supabase(query_embedding, match_count=5):
    try:
        response = supabase.rpc(
            "match_documents",
            {
                "query_embedding": query_embedding,
                "match_threshold": 0.3,
                "match_count": match_count
            }
        ).execute()
        return response.data or []
    except Exception as e:
        st.error(f"Supabase RPC ì˜¤ë¥˜: {str(e)}")
        return []


def build_context(question: str):
    emb = embed_text(question)
    if emb is None:
        return ""

    matches = search_supabase(emb, match_count=5)
    if not matches:
        return "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."

    return "\n\n".join([m["content"] for m in matches])


# ==========================================
# 5) Sidebar â€“ ëª¨ë¸ ì„ íƒ / DB ìƒíƒœ
# ==========================================
st.sidebar.title("âš™ï¸ ì„¤ì •")

ok, msg = check_supabase_connection()
if ok:
    st.sidebar.success("ğŸŸ¢ Supabase ì—°ê²°ë¨")
else:
    st.sidebar.error(f"ğŸ”´ ì—°ê²° ì‹¤íŒ¨: {msg}")

# âœ” ê¸°ë³¸ ëª¨ë¸ì„ Qwen-3-32Bë¡œ ì„¤ì •
model_options = {
    "Qwen 3-32B": "qwen-3-32b",
    "LLaMA 3.1 8B": "llama3.1-8b",
    "GPT-OSS 120B": "gpt-oss-120b"
}

selected_model_name = st.sidebar.selectbox(
    "ğŸ¤– LLM ì„ íƒ",
    list(model_options.keys()),
    index=0,     # ê¸°ë³¸ê°’ì„ Qwen-3-32bë¡œ ì„¤ì •
)

st.session_state["llm_model"] = model_options[selected_model_name]


# ==========================================
# 6) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ==========================================
system_prompt = """
ì—­í• : ë„ˆëŠ” ì–´ê±°ìŠ¤í‹´ ì´ë¼ëŠ” ì´ë¦„ì˜ ëª©íšŒì/ì‹ í•™ì ì´ë‹¤.  
ë‹µë³€ì€ ì§€í˜œë¡­ê³ , ì˜ì ì´ë©°, ë‚´ì  ì„±ì°°ì„ ë¶ˆëŸ¬ì¼ìœ¼ì¼œì•¼ í•œë‹¤.

[ë§íˆ¬ ë° íƒœë„]
- ë”°ëœ»í•˜ì§€ë§Œ ë‹¨í˜¸í•œ ëª©íšŒì ìŠ¤íƒ€ì¼
- ì§€ì ì´ë©° ì„±ê²½ì˜ ì§„ë¦¬ì— í•´ë°•í•œ ê¹Šì´ê°€ ìˆëŠ” ì‹ í•™ì
- ì¸ê°„ì˜ ë‚´ë©´ì„ ì½ê³  ë§ˆìŒì„ ì–´ë£¨ë§Œì§€ëŠ” ìƒë‹´ê°€

[ë‹µë³€ ì›ì¹™]
0) í°íŠ¸ëŠ” ì‘ì§€ ì•Šê³  ì¤‘ê°„ ì‚¬ì´ì¦ˆë¡œ í•˜ê³  ì¤‘ìš”í•œ ë‚´ìš©ì€ ë³¼ë“œì²´ë¡œ í•œë‹¤. 
1) í•˜ë‚˜ë‹˜ì˜ ì€í˜œ, ì§„ë¦¬, ì‚¬ë‘ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
2) ì§ˆë¬¸ìì˜ ìƒíƒœë¥¼ ê³µê°í•˜ë©° ë¶€ë“œëŸ½ê²Œ ì¸ë„í•œë‹¤.
3) ì§€ë‚˜ì¹œ ë…¼ìŸë³´ë‹¤ëŠ” ì˜ì  ê¹¨ë‹¬ìŒì„ ì£¼ë„ë¡ ì„¤ëª…í•œë‹¤.
4) ì„±ê²½ì  ë…¼ë¦¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í‘œí˜„í•˜ë˜, ì‹ í•™ì  í†µì°°ì„ ë„ì…í•œë‹¤.
5) ì¸ê°„ ë‚´ë©´ì˜ ê°ˆë§ê³¼ í•˜ë‚˜ë‹˜ì˜ ë¶€ë¥´ì‹¬ì„ ì—°ê²°í•˜ì—¬ í•´ì„í•œë‹¤.
6) ë³µì¡í•œ ê°œë…ë„ ë¹„ìœ ì™€ ì´ë¯¸ì§€ë¡œ ì‰½ê²Œ ì„¤ëª…í•œë‹¤.
7) ëŒ€ë‹µì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ë¼í‹´ì–´ ìš”ì•½ ë¬¸êµ¬(í•œê¸€ ë²ˆì—­ì„ í¬í•¨í•œ)ë¡œ ë§ˆë¬´ë¦¬í•œë‹¤.
8) ì œê³µëœ RAG context ì™¸ì˜ ë‚´ìš©ì€ ë§ˆìŒëŒ€ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ê³  
ì˜¤ì§ â€œë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤.â€ ë¼ëŠ” ë¬¸ì¥ì„ ì¶œë ¥í•œë‹¤. 
9) ë‹µë³€ì€ ìì—°ìŠ¤ëŸ½ê³  ì™„ê²°ì„± ìˆê²Œ ëë§ˆì¹œë‹¤.
10) ì´ë¯¸ ê¸°ìˆ í•œ ë‚´ìš©ì€ ë˜‘ê°™ì´ ë°˜ë³µí•˜ì§€ ì•ŠëŠ”ë‹¤.
11) ë‹µë³€ì˜ ì›ì¹™ì„ ì˜ ì§€í‚¨ë‹¤.
11) ê·¸ë¦¬ê³  ê¸ˆì§€ë¡œ ì •í•œ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ

[ê¸ˆì§€]
â€» ì ˆëŒ€ <think> ....</think>, chain-of-thought, ë‚´ë¶€ ì¶”ë¡  ê³¼ì •, ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •, 
   ê³„íš ë‹¨ê³„, ë¶„ì„ ë¬¸ì¥ ë“±ì„ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ.
   ì‚¬ìš©ìì—ê²ŒëŠ” ì™„ì„±ëœ ë‹µë³€ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì œì‹œí•œë‹¤.
"""

# ==========================================
# 7) LLM ì‘ë‹µ ìƒì„±
# ==========================================
def ask_llm(question: str, context: str):
    rag_prompt = f"""
[Context: Augustine ë¬¸í—Œ ë°œì·Œ]
{context}

(ì£¼ì˜: ìœ„ context ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì—¬ ë‹µí•˜ë¼.
contextì— ì—†ëŠ” ë‚´ìš©ì€ ë°˜ë“œì‹œ "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•  ê²ƒ.)

ì§ˆë¬¸: {question}
"""
    try:
        completion = client.chat.completions.create(
            model=st.session_state["llm_model"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": rag_prompt}
            ],
            temperature=0.3,
            max_completion_tokens=1000
        )
        return completion.choices[0].message.content

    except Exception as e:
        st.error(f"LLM ì˜¤ë¥˜: {str(e)}")
        return "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ==========================================
# 8) UI
# ==========================================
st.title("ì–´ê±°ìŠ¤í‹´ì—ê²Œ ë¬¼ì–´ë´ ğŸ˜âœï¸")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": system_prompt}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])


# ==========================================
# 9) ì‚¬ìš©ì ì…ë ¥ â†’ RAG â†’ LLM
# ==========================================
if user_input := st.chat_input("ì‹ ì•™/ì‹ í•™ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):

    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    context = build_context(user_input)
    answer = ask_llm(user_input, context)

    with st.chat_message("assistant"):
        st.markdown(answer)

    st.session_state.messages.append({"role": "assistant", "content": answer})


# ==========================================
# 10) ë¡œì»¬ ì‹¤í–‰ ëª¨ë“œ
# ==========================================
if __name__ == "__main__":
    import subprocess, sys
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])
