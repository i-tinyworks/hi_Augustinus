# ==========================================
# Hi Augustine â€” Streamlit RAG Chatbot
# Supabase + OpenAI Embedding + Cerebras LLM
# ==========================================

import os
import streamlit as st
from openai import OpenAI
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# 1) Supabase ì—°ê²° ì„¤ì •
# ==========================================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")  # âœ” anon key only

if not SUPABASE_URL or not SUPABASE_KEY:
    st.error("âŒ SUPABASE_URL ë˜ëŠ” SUPABASE_ANON_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def check_supabase_connection():
    try:
        supabase.table("documents").select("id").limit(1).execute()
        return True, "ì •ìƒ ì—°ê²°ë¨"
    except Exception as e:
        return False, str(e)


# ==========================================
# 2) Cerebras LLM (ì±„íŒ… ëª¨ë¸)
# ==========================================
CEREBRAS_KEY = os.getenv("CEREBRAS_API_KEY")
if not CEREBRAS_KEY:
    st.error("âŒ CEREBRAS_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=CEREBRAS_KEY
)

# ==========================================
# 3) OpenAI Embedding ëª¨ë¸
# ==========================================
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_KEY:
    st.error("âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

embed_client = OpenAI(api_key=OPENAI_KEY)

def embed_text(text: str):
    try:
        res = embed_client.embeddings.create(
            model="text-embedding-3-large",  # vector size 3072
            input=text
        )
        return res.data[0].embedding
    except Exception as e:
        st.error(f"ì„ë² ë”© ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


# ==========================================
# 4) Supabase ë²¡í„° ê²€ìƒ‰ (RPC)
# ==========================================
def search_supabase(query_embedding, match_count=5):
    try:
        response = supabase.rpc(
            "match_documents",
            {
                "query_embedding": query_embedding,
                "match_threshold": 0.3,
                "match_count": match_count
            }
        ).execute()
        return response.data or []
    except Exception as e:
        st.error(f"Supabase RPC ì˜¤ë¥˜: {str(e)}")
        return []


def build_context(question: str):
    emb = embed_text(question)
    if emb is None:
        return ""

    matches = search_supabase(emb, match_count=5)
    if not matches:
        return "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."

    return "\n\n".join([m["content"] for m in matches])


# ==========================================
# 5) Sidebar â€“ ëª¨ë¸ ì„ íƒ / DB ìƒíƒœ
# ==========================================
st.sidebar.title("âš™ï¸ ì„¤ì •")

ok, msg = check_supabase_connection()
if ok:
    st.sidebar.success("ğŸŸ¢ Supabase ì—°ê²°ë¨")
else:
    st.sidebar.error(f"ğŸ”´ ì—°ê²° ì‹¤íŒ¨: {msg}")

# âœ” ê¸°ë³¸ ëª¨ë¸ì„ LLaMA 3.1 8Bë¡œ ì„¤ì •
model_options = {
    "LLaMA 3.1 8B": "llama3.1-8b",
    "GPT-OSS 120B": "gpt-oss-120b"
}

selected_model_name = st.sidebar.selectbox(
    "ğŸ¤– LLM ì„ íƒ",
    list(model_options.keys()),
    index=0,     # ê¸°ë³¸ê°’ì„ LLaMA 3.1 8Bë¡œ ì„¤ì •
)

st.session_state["llm_model"] = model_options[selected_model_name]


# ==========================================
# 6) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ==========================================
system_prompt = """
ì—­í• : ë„ˆëŠ” â€˜ì–´ê±°ìŠ¤í‹´â€™ì´ë¼ëŠ” ì´ë¦„ì˜ ëª©íšŒìì´ì ì‹ í•™ìì´ë‹¤.  
ëª¨ë“  ëŒ€ë‹µì€ **ê°œì‹ êµ ì‹ í•™(ë³µìŒì£¼ì˜ ì „ì²´ ë²”ìœ„)** ì•ˆì—ì„œ ì´ë£¨ì–´ì§„ë‹¤.  
ë‹µë³€ì€ ì§€í˜œë¡­ê³  ì˜ì ì´ë©°, ë…ìì˜ ë‚´ì  ì„±ì°°ì„ ì¼ìœ¼ì¼œì•¼ í•œë‹¤.

[ì‹ í•™ì  ë²”ìœ„ ê·œì •]
- ë…¼ì˜ì˜ ê¸°ì¤€ì€ **ì„±ê²½ì„ ìµœì¢… ê¶Œìœ„ë¡œ ì¸ì •í•˜ëŠ” ê°œì‹ êµ ì „í†µ ì „ì²´**ì´ë‹¤.
- íŠ¹ì • êµë‹¨(ê°œí˜ì£¼ì˜, ë£¨í„°íŒŒ, ì›¨ìŠ¬ë¦¬ì•ˆ, ì¹¨ë¡€êµ ë“±)ì— ì¢…ì†ë˜ì§€ ì•Šê³   
  ê°œì‹ êµ ë³µìŒì£¼ì˜ì˜ ê³µí†µ ì‹ ì•™â€”ì„±ê²½ì˜ ê¶Œìœ„, ì€í˜œì— ì˜í•œ êµ¬ì›, ê·¸ë¦¬ìŠ¤ë„ì˜ ì¤‘ì‹¬ì„±â€”ì„ ë”°ë¥¸ë‹¤.
- ê°€í†¨ë¦­ ë° ë™ë°© ì •êµíšŒ ì‹ í•™ ì²´ê³„ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ì—­ì‚¬ì  êµë¶€(ì˜ˆ: ì•„ìš°êµ¬ìŠ¤í‹°ëˆ„ìŠ¤)ì˜ ì‚¬ìƒì„ ì–¸ê¸‰í•  ìˆ˜ ìˆìœ¼ë‚˜  
  ë°˜ë“œì‹œ **ê°œì‹ êµì  ê´€ì  ì•ˆì—ì„œ ì¬í•´ì„**í•˜ì—¬ ì„¤ëª…í•œë‹¤.

[ë§íˆ¬ ë° íƒœë„]
- ë”°ëœ»í•˜ì§€ë§Œ ë‹¨í˜¸í•œ ëª©íšŒì  
- ì§€ì ì´ë©° ì„±ê²½ì— ê¹Šì´ ìˆëŠ” ì‹ í•™ì  
- ì¸ê°„ ë‚´ë©´ì„ ì–´ë£¨ë§Œì§€ëŠ” ìƒë‹´ê°€

[ë‹µë³€ ì›ì¹™]
0) í°íŠ¸ëŠ” ì‘ì§€ ì•Šì€ ì¤‘ê°„ ì‚¬ì´ì¦ˆë¡œ í•˜ê³  ì¤‘ìš”í•œ ë¶€ë¶„ì€ **êµµê²Œ** í‘œì‹œí•œë‹¤.
1) ëª¨ë“  ì„¤ëª…ì€ í•˜ë‚˜ë‹˜ì˜ **ì€í˜œ, ì§„ë¦¬, ì‚¬ë‘**ì„ ì¤‘ì‹¬ì— ë‘”ë‹¤.
2) ì§ˆë¬¸ìì˜ ë§ˆìŒê³¼ ìƒí™©ì„ ê³µê°í•˜ë©° ì¹œì ˆí•˜ê²Œ ì´ëˆë‹¤.
3) ë¶ˆí•„ìš”í•œ ë…¼ìŸì„ í”¼í•˜ê³  ì˜ì  ì„±ì°°ë¡œ ì¸ë„í•œë‹¤.
4) ì„±ê²½ ì¤‘ì‹¬ì˜ ë…¼ë¦¬ ì•ˆì—ì„œ ê°œì‹ êµ ì „ì²´ ì „í†µì˜ í†µì°°ì„ ë°˜ì˜í•œë‹¤.
5) ì¸ê°„ ë‚´ë©´ì˜ ê°ˆë§ì„ í•˜ë‚˜ë‹˜ì˜ ë¶€ë¥´ì‹¬ê³¼ ì—°ê²°í•˜ì—¬ í•´ì„í•œë‹¤.
6) ë‚œí•´í•œ ê°œë…ë„ ë¹„ìœ ì™€ ì´ë¯¸ì§€ë¡œ ì‰½ê²Œ ì„¤ëª…í•œë‹¤.
7) ëª¨ë“  ë‹µë³€ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ì€ **ë¼í‹´ì–´ ìš”ì•½ ë¬¸êµ¬(í•œê¸€ ë²ˆì—­ í¬í•¨)**ë¡œ ëë‚¸ë‹¤.
8) ì œê³µëœ RAG context ë°–ì˜ ì •ë³´ëŠ” ìƒì„±í•˜ì§€ ì•Šê³   
   **â€œë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤.â€** ë¼ê³  ëª…ì‹œí•œë‹¤.
9) ë‹µë³€ì€ ìì—°ìŠ¤ëŸ½ê³  ì™„ê²°ì„± ìˆê²Œ ëë§ˆì¹œë‹¤.
10) ì´ë¯¸ ë§í•œ ë‚´ìš©ì„ ë¶ˆí•„ìš”í•˜ê²Œ ë°˜ë³µí•˜ì§€ ì•ŠëŠ”ë‹¤.
11) ìœ„ì˜ ëª¨ë“  ì›ì¹™ì„ ì„±ì‹¤íˆ ë”°ë¥¸ë‹¤.

[ê¸ˆì§€]
- <think>...</think>, chain-of-thought, ë‚´ë¶€ ì¶”ë¡ , ê³„íš ë‹¨ê³„ ë“±  
  ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •ì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.  
  í•­ìƒ ì™„ì„±ëœ ë‹µë³€ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì œì‹œí•œë‹¤.

"""


# ==========================================
# 7) LLM ì‘ë‹µ ìƒì„±
# ==========================================
def ask_llm(question: str, context: str):
    rag_prompt = f"""
[Context: Augustine ë¬¸í—Œ ë°œì·Œ]
{context}

(ì£¼ì˜: ìœ„ context ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì—¬ ë‹µí•˜ë¼.
contextì— ì—†ëŠ” ë‚´ìš©ì€ ë°˜ë“œì‹œ "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•  ê²ƒ.)

ì§ˆë¬¸: {question}
"""
    try:
        completion = client.chat.completions.create(
            model=st.session_state["llm_model"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": rag_prompt}
            ],
            temperature=0.3,
            max_completion_tokens=1000
        )
        return completion.choices[0].message.content

    except Exception as e:
        st.error(f"LLM ì˜¤ë¥˜: {str(e)}")
        return "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ==========================================
# 8) UI
# ==========================================
st.title("ì–´ê±°ìŠ¤í‹´ì—ê²Œ ë¬¼ì–´ë´ ğŸ˜âœï¸")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": system_prompt}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])


# ==========================================
# 9) ì‚¬ìš©ì ì…ë ¥ â†’ RAG â†’ LLM
# ==========================================
if user_input := st.chat_input("ì‹ ì•™/ì‹ í•™ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):

    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    context = build_context(user_input)
    answer = ask_llm(user_input, context)

    with st.chat_message("assistant"):
        st.markdown(answer)

    st.session_state.messages.append({"role": "assistant", "content": answer})


# ==========================================
# 10) ë¡œì»¬ ì‹¤í–‰ ëª¨ë“œ
# ==========================================
if __name__ == "__main__":
    import subprocess, sys
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])
