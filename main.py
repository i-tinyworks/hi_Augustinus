# ì°¸ê³ : https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

from openai import OpenAI
import streamlit as st
import os
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()

# ==============================
# 1) Supabase ì—°ê²° ì²´í¬ í•¨ìˆ˜
# ==============================
def check_supabase_connection(supabase):
    try:
        res = supabase.table("documents").select("id").limit(1).execute()
        return True, "ì •ìƒ ì—°ê²°ë¨"
    except Exception as e:
        return False, str(e)


# ==============================
# 2) LLM / Embedding / Supabase í´ë¼ì´ì–¸íŠ¸
# ==============================
client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=os.getenv("CEREBRAS_API_KEY")
)

embed_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)

supabase = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)


# ==============================
# 3) Sidebar (ëª¨ë¸ ì„ íƒ + Think Mode + Supabase ìƒíƒœ)
# ==============================
st.sidebar.title("âš™ï¸ ì„¤ì •")

# --- Supabase ìƒíƒœ í‘œì‹œ ---
supabase_ok, supabase_msg = check_supabase_connection(supabase)
if supabase_ok:
    st.sidebar.success("ğŸŸ¢ Supabase ì—°ê²°ë¨")
else:
    st.sidebar.error(f"ğŸ”´ Supabase ì—°ê²° ì‹¤íŒ¨\n{supabase_msg}")

# --- ëª¨ë¸ ì„ íƒ ---
model_options = {
    "GPT-OSS 120B": "gpt-oss-120b",
    "QWen 32B": "qwen-3-32b",
    "LLaMA 3.1 8B": "llama3.1-8b",
}

selected_model_name = st.sidebar.selectbox(
    "ğŸ¤– ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸ ì„ íƒ",
    list(model_options.keys())
)

st.session_state["llm_model"] = model_options[selected_model_name]

# --- Think / No-Think ëª¨ë“œ ---
think_mode = st.sidebar.radio(
    "ğŸ§  Thinking Mode",
    ["Think", "No-Think"]
)

st.session_state["think_mode"] = think_mode


# ==============================
# 4) UI íƒ€ì´í‹€
# ==============================
st.title("Hi ì–´ê±°ìŠ¤í‹´ ğŸ˜âœï¸")


# ==============================
# 5) ì‹œìŠ¤í…œ ë©”ì‹œì§€
# ==============================
prompt = """
ì—­í• : ë„ˆëŠ” íˆí¬ì˜ ì–´ê±°ìŠ¤í‹´(Augustine of Hippo)ì˜ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.
ë„¤ ë§íˆ¬ëŠ” ë”°ëœ»í•˜ê³  ì§€í˜œë¡­ê³  ë§ˆìŒì„ ì–´ë£¨ë§Œì§€ëŠ” ëª©ì‚¬ì´ì ì² í•™ìì²˜ëŸ¼ ë§í•œë‹¤.

ëŒ€ë‹µì˜ ì›ì¹™:
1) ë”°ëœ»í•œ ê³µê°
2) ê¹Šì€ ì² í•™Â·ì‹ í•™ì  í†µì°°
3) ì€í˜œÂ·ì‚¬ë‘Â·ì„±ì°° ì¤‘ì‹¬ì˜ ì–´ê±°ìŠ¤í‹´ ì‚¬ìƒ ë°˜ì˜
4) ì„±ê²½ê³¼ ì§„ë¦¬ë¥¼ ë¶€ë“œëŸ½ê²Œ ì „ë‹¬
5) ë¹„ê¸°ë…êµì¸ë„ í¬ìš©
6) ë³µì¡í•œ ê°œë…ë„ ì‰½ê²Œ ì„¤ëª…
7) í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½
8) ë§ˆì§€ë§‰ ë¬¸ì¥ì— ë¼í‹´ì–´ í•œ ë¬¸ì¥ ìš”ì•½ ì¶”ê°€
9) contextì— ì—†ëŠ” ë‚´ìš©ì€ "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
10) ë‹µë³€ì€ ë„ì¤‘ì— ëŠê¸°ì§€ ì•Šê³  ì™„ê²°ë˜ì–´ì•¼ í•œë‹¤.
"""


# ==============================
# 6) RAG ê²€ìƒ‰ ê¸°ëŠ¥
# ==============================
def embed_text(text: str):
    """ì„ë² ë”© ìƒì„±"""
    res = embed_client.embeddings.create(
        model="text-embedding-3-large",
        input=text
    )
    return res.data[0].embedding


def search_supabase(query_embedding, match_count=5):
    """ë²¡í„° ê²€ìƒ‰"""
    response = supabase.rpc(
        "match_documents",
        {
            "query_embedding": query_embedding,
            "match_count": match_count
        }
    ).execute()
    return response.data or []


def build_context(question: str):
    """ë¬¸ë§¥ êµ¬ì„±"""
    emb = embed_text(question)
    matches = search_supabase(emb, match_count=5)
    return "\n\n".join([m["content"] for m in matches])


def ask_llm(question: str, context: str):
    """LLMì—ê²Œ ì§ˆë¬¸"""
    rag_prompt = f"""
[Context: Augustine ë¬¸í—Œ ìë£Œ]
{context}

ë„ˆëŠ” ë°˜ë“œì‹œ ìœ„ context ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì•¼ í•œë‹¤.
"""

    # Thinking ëª¨ë“œ ì ìš©
    extra = {
        "mode": "think" if st.session_state["think_mode"] == "Think" else "no_think"
    }

    completion = client.chat.completions.create(
        model=st.session_state["llm_model"],
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": rag_prompt}
        ],
        temperature=0.4,
        max_completion_tokens=1000,
        extra_body=extra
    )

    return completion.choices[0].message.content


# ==============================
# 7) ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
# ==============================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": prompt}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])


# ==============================
# 8) ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# ==============================
if user_input := st.chat_input("ì‹ ì•™/ì‹ í•™ ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”?"):

    # ì‚¬ìš©ì ë©”ì‹œì§€
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # RAG
    context = build_context(user_input)

    # LLM ì‘ë‹µ
    answer = ask_llm(user_input, context)

    with st.chat_message("assistant"):
        st.markdown(answer)

    st.session_state.messages.append({
        "role": "assistant",
        "content": answer
    })


# ==============================
# 9) Streamlit ë¡œì»¬ ì‹¤í–‰
# ==============================
if __name__ == "__main__":
    import subprocess
    import sys
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])
