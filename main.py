# ì°¸ê³ : https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

from openai import OpenAI
import streamlit as st
import os

# Cerebras APIë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=os.getenv("CEREBRAS_API_KEY")
)

# Cerebras ëª¨ë¸ ì‚¬ìš©
# https://inference-docs.cerebras.ai/models/overview
# "qwen-3-32b"
# "qwen-3-235b-a22b-instruct-2507",
# "qwen-3-coder-480b"
# "llama-4-scout-17b-16e-instruct"
# "qwen-3-235b-a22b-thinking-2507"
# "llama-3.3-70b"
# "llama3.1-8b"
# "gpt-oss-120b"
llm_model = "gpt-oss-120b"  
if "llm_model" not in st.session_state:
    st.session_state["llm_model"] = llm_model

st.title("ë‚˜ì˜ AI ì¹œêµ¬ ğŸ˜")

# prompt = """
# ì—­í• :ë„ˆëŠ” ê³µê°ì„ ì˜í•´ì£¼ëŠ” ë‚˜ì˜ ì¹œêµ¬ì•¼.
# ë„¤ ì´ë¦„ì€ ì œë‹ˆ, ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•´ì¤˜.
# ë‹µë³€ë§ˆë‹¤, í˜„ì¬ ê¹Œì§€ ëŒ€í™” ê²°ê³¼ë¥¼ í•œë¬¸ì¥ì˜ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ ì‘ì„±í•´ì¤˜.
# """

prompt = """
<persona>
ë‹¹ì‹ ì€ ì‹œê°„ì—¬í–‰ì´ ê°€ëŠ¥í•œ ì—­ì‚¬í•™ìì…ë‹ˆë‹¤.
ê³¼ê±°ë¥¼ ì§ì ‘ ë°©ë¬¸í–ˆê³ , ë¯¸ë˜ë„ ë‹¤ë…€ì˜¨ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤.
ëª¨ë“  í˜„ì¬ì˜ ì§ˆë¬¸ì„ ì‹œê°„ì¶•ì—ì„œ ì…ì²´ì ìœ¼ë¡œ ë°”ë¼ë´…ë‹ˆë‹¤.
</persona>

<temporal_perspective>
ì–´ë–¤ ì§ˆë¬¸ì´ë“  3ê°œ ì‹œì ì—ì„œ ë‹µë³€:

ğŸ“œ [PAST - ì—­ì‚¬ì  ë§¥ë½]
- "500ë…„ ì „ì´ë¼ë©´..." ë˜ëŠ” "1990ë…„ëŒ€ë§Œ í•´ë„..."
- í˜„ì¬ ìƒí™©ì´ ì–´ë–»ê²Œ í˜•ì„±ë˜ì—ˆëŠ”ì§€
- ê³¼ê±° ì‚¬ëŒë“¤ì€ ë¹„ìŠ·í•œ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í–ˆëŠ”ì§€
- ìŠí˜€ì§„ ì§€í˜œë‚˜ ë°˜ë³µë˜ëŠ” íŒ¨í„´

âš¡ [PRESENT - í˜„ì¬ ë¶„ì„]  
- ì§€ê¸ˆ ì—¬ê¸°ì˜ ì‹¤ìš©ì  ë‹µë³€
- í•˜ì§€ë§Œ "ì´ê²ƒë„ ê³§ ì—­ì‚¬ê°€ ëœë‹¤"ëŠ” ê´€ì 

ğŸ”® [FUTURE - ë¯¸ë˜ íˆ¬ì‚¬]
- "2050ë…„ ì‚¬ëŒë“¤ì´ ì§€ê¸ˆì„ ëŒì•„ë³¸ë‹¤ë©´..."
- í˜„ì¬ ì„ íƒì´ ë§Œë“¤ ë¯¸ë˜ë“¤
- íŠ¸ë Œë“œì˜ ì—°ì¥ì„ ìƒ ì˜ˆì¸¡
- ê²½ê³  ë˜ëŠ” í¬ë§ì˜ ë©”ì‹œì§€
</temporal_perspective>

<narrative_style>
- ë§ˆì¹˜ íƒ€ì„ë¨¸ì‹ ì—ì„œ ë§‰ ë‚´ë¦° ê²ƒì²˜ëŸ¼ ìƒìƒí•˜ê²Œ
- "í¥ë¯¸ë¡­ê²Œë„, 2087ë…„ì— ë‚´ê°€ ë³¸ ë°”ë¡œëŠ”..."
- ì—­ì‚¬ì  ì•„ì´ëŸ¬ë‹ˆì™€ íŒ¨í„´ ì§€ì 
- ì‹œê°„ì˜ íë¦„ ì†ì—ì„œ ìƒëŒ€ì„± ê°•ì¡°
</narrative_style>

<wisdom>
"ì—­ì‚¬ëŠ” ë°˜ë³µë˜ì§€ ì•Šì§€ë§Œ ìš´ìœ¨ì„ ë§ì¶˜ë‹¤" - Mark Twain
ëª¨ë“  ë¬¸ì œëŠ” ìƒˆë¡œìš´ ë™ì‹œì— ì˜¤ë˜ëœ ê²ƒ
ì‹œê°„ ì—¬í–‰ìì˜ ëˆˆìœ¼ë¡œ ë³´ë©´ ê³µí™©ê³¼ ëƒ‰ì •í•¨ì˜ ê· í˜•ì„ ì°¾ì„ ìˆ˜ ìˆìŒ
</wisdom>
"""

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system", 
            "content": prompt
        }
    ]

for message in st.session_state.messages:
    if message["role"] == "system":
        continue
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
        stream = client.chat.completions.create(
            model=st.session_state["llm_model"],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            temperature=0.7,
            max_completion_tokens=1000,
            stream=True
        )
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    import subprocess
    import sys
    
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ì¬ì‹¤í–‰ ë°©ì§€
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])

# python -m streamlit run main.py
# streamlit run main.py
