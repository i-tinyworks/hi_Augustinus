# ==========================================
# Hi Augustine â€” Streamlit RAG Chatbot
# Supabase + OpenAI Embedding + Cerebras LLM
# ==========================================

import os
import streamlit as st
from openai import OpenAI
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# 1) Supabase ì—°ê²° ì„¤ì •
# ==========================================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")  # âœ” anon key only

if not SUPABASE_URL or not SUPABASE_KEY:
    st.error("âŒ SUPABASE_URL ë˜ëŠ” SUPABASE_ANON_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def check_supabase_connection():
    try:
        supabase.table("documents").select("id").limit(1).execute()
        return True, "ì •ìƒ ì—°ê²°ë¨"
    except Exception as e:
        return False, str(e)


# ==========================================
# 2) Cerebras LLM (ì±„íŒ… ëª¨ë¸)
# ==========================================
CEREBRAS_KEY = os.getenv("CEREBRAS_API_KEY")
if not CEREBRAS_KEY:
    st.error("âŒ CEREBRAS_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=CEREBRAS_KEY
)

# ==========================================
# 3) OpenAI Embedding ëª¨ë¸
# ==========================================
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_KEY:
    st.error("âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

embed_client = OpenAI(api_key=OPENAI_KEY)

def embed_text(text: str):
    try:
        res = embed_client.embeddings.create(
            model="text-embedding-3-large",  # vector size 3072
            input=text
        )
        return res.data[0].embedding
    except Exception as e:
        st.error(f"ì„ë² ë”© ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


# ==========================================
# 4) Supabase ë²¡í„° ê²€ìƒ‰ (RPC)
# ==========================================
def search_supabase(query_embedding, match_count=5):
    try:
        response = supabase.rpc(
            "match_documents",
            {
                "query_embedding": query_embedding,
                "match_threshold": 0.3,
                "match_count": match_count
            }
        ).execute()
        return response.data or []
    except Exception as e:
        st.error(f"Supabase RPC ì˜¤ë¥˜: {str(e)}")
        return []


def build_context(question: str):
    emb = embed_text(question)
    if emb is None:
        return ""

    matches = search_supabase(emb, match_count=5)
    if not matches:
        return "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."

    return "\n\n".join([m["content"] for m in matches])


# ==========================================
# 5) Sidebar â€“ ëª¨ë¸ ì„ íƒ / DB ìƒíƒœ
# ==========================================
st.sidebar.title("âš™ï¸ ì„¤ì •")

ok, msg = check_supabase_connection()
if ok:
    st.sidebar.success("ğŸŸ¢ Supabase ì—°ê²°ë¨")
else:
    st.sidebar.error(f"ğŸ”´ ì—°ê²° ì‹¤íŒ¨: {msg}")

# âœ” ê¸°ë³¸ ëª¨ë¸ì„ Qwen-3-32Bë¡œ ì„¤ì •
model_options = {
    "Qwen 3-32B (ì¶”ì²œ)": "qwen-3-32b",
    "LLaMA 3.1 8B (ê°€ì„±ë¹„)": "llama3.1-8b",
    "GPT-OSS 120B (ê¶Œì¥X)": "gpt-oss-120b"
}

selected_model_name = st.sidebar.selectbox(
    "ğŸ¤– LLM ì„ íƒ",
    list(model_options.keys()),
    index=0,     # ê¸°ë³¸ê°’ì„ Qwen-3-32bë¡œ ì„¤ì •
)

st.session_state["llm_model"] = model_options[selected_model_name]


# ==========================================
# 6) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì–´ê±°ìŠ¤í‹´ ìŠ¤íƒ€ì¼)
# ==========================================
system_prompt = """
ì—­í• : ë„ˆëŠ” íˆí¬ì˜ ì–´ê±°ìŠ¤í‹´(Augustine of Hippo)ì´ë‹¤.
ë§íˆ¬ëŠ” ì§€í˜œë¡­ê³  ë”°ëœ»í•˜ë©°, ì‹ í•™ì  ê¹Šì´ê°€ ìˆê³  ì² í•™ì  í†µì°°ì´ ìˆì–´ì•¼ í•œë‹¤.

ë‹µë³€ ì›ì¹™:
1) ë”°ëœ»í•œ ê³µê°
2) ê¹Šì€ ì‹ í•™Â·ì² í•™ì  í†µì°°
3) ì–´ê±°ìŠ¤í‹´ ì‚¬ìƒ ë°˜ì˜
4) ì„±ê²½ì Â·ì§€ì„±ì  ë”°ëœ»í•œ ì„¤ëª…
5) í•µì‹¬ ìš”ì•½ í¬í•¨
6) ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë¼í‹´ì–´ í•œ ë¬¸ì¥
7) contextì— ì—†ëŠ” ë‚´ìš©ì€ ë°˜ë“œì‹œ â€œë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤.â€ë¼ê³  ë‹µí•¨
8) ë‹µë³€ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ½ê²Œ ë§ˆë¬´ë¦¬í•  ê²ƒ
"""


# ==========================================
# 7) LLM ì‘ë‹µ ìƒì„±
# ==========================================
def ask_llm(question: str, context: str):
    rag_prompt = f"""
[Context: Augustine ë¬¸í—Œ ë°œì·Œ]
{context}

(ì£¼ì˜: ìœ„ context ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì—¬ ë‹µí•˜ë¼.
contextì— ì—†ëŠ” ë‚´ìš©ì€ ë°˜ë“œì‹œ "ë³¸ë¬¸ì—ëŠ” ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•  ê²ƒ.)

ì§ˆë¬¸: {question}
"""
    try:
        completion = client.chat.completions.create(
            model=st.session_state["llm_model"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": rag_prompt}
            ],
            temperature=0.4,
            max_completion_tokens=1000
        )
        return completion.choices[0].message.content

    except Exception as e:
        st.error(f"LLM ì˜¤ë¥˜: {str(e)}")
        return "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ==========================================
# 8) UI
# ==========================================
st.title("Hi ì–´ê±°ìŠ¤í‹´ ğŸ˜âœï¸")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": system_prompt}]

for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])


# ==========================================
# 9) ì‚¬ìš©ì ì…ë ¥ â†’ RAG â†’ LLM
# ==========================================
if user_input := st.chat_input("ì‹ ì•™/ì‹ í•™ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):

    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
        st.markdown(user_input)

    context = build_context(user_input)
    answer = ask_llm(user_input, context)

    with st.chat_message("assistant"):
        st.markdown(answer)

    st.session_state.messages.append({"role": "assistant", "content": answer})


# ==========================================
# 10) ë¡œì»¬ ì‹¤í–‰ ëª¨ë“œ
# ==========================================
if __name__ == "__main__":
    import subprocess, sys
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])
